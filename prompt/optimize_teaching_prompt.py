#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼–ç¨‹æ•™å­¦åŠ©æ‰‹ - è‡ªåŠ¨åŒ–æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ
åŸºäº PhaseEvo + Autoprompt æ–¹æ³•
"""

import sys
import os

# æ·»åŠ å¿…è¦çš„è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)

# æ’å…¥åˆ°è·¯å¾„æœ€å‰é¢ï¼Œç¡®ä¿ä¼˜å…ˆå¯¼å…¥
sys.path.insert(0, os.path.join(parent_dir, 'EvoAutoprompt'))
sys.path.insert(0, parent_dir)
sys.path.insert(0, current_dir)

from teaching_optimizer_wrapper import TeachingOptimizer  # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨
from teaching_evaluator import TeachingPromptEvaluator, prepare_teaching_dataset
from llm import volcengine_ark_llm_eval
from datetime import datetime
import json

def main():
    print("=" * 70)
    print("ç¼–ç¨‹æ•™å­¦åŠ©æ‰‹ - è‡ªåŠ¨åŒ–æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ")
    print("åŸºäº PhaseEvo + Autoprompt æ–¹æ³•")
    print("=" * 70)
    
    # åˆ›å»ºç»“æœç›®å½•ï¼ˆåœ¨å½“å‰ç›®å½•ä¸‹çš„qwen-teaching-chatbotä¸­ï¼‰
    results_dir = os.path.join(current_dir, 'qwen-teaching-chatbot - å‰¯æœ¬', 'results')
    os.makedirs(results_dir, exist_ok=True)
    
    # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\n[é˜¶æ®µ1/4] å‡†å¤‡æµ‹è¯•æ•°æ®é›†...")
    test_data = prepare_teaching_dataset()
    print(f"âœ“ å·²å‡†å¤‡ {len(test_data)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # ç»Ÿè®¡ä¿¡æ¯
    syntax_count = sum(1 for _, r in test_data.iterrows() if r['error_type'] == 'syntax')
    runtime_count = sum(1 for _, r in test_data.iterrows() if r['error_type'] == 'runtime')
    logical_count = sum(1 for _, r in test_data.iterrows() if r['error_type'] == 'logical')
    conceptual_count = sum(1 for _, r in test_data.iterrows() if r['error_type'] == 'conceptual')
    
    beginner_count = sum(1 for _, r in test_data.iterrows() if r['difficulty'] == 'beginner')
    intermediate_count = sum(1 for _, r in test_data.iterrows() if r['difficulty'] == 'intermediate')
    advanced_count = sum(1 for _, r in test_data.iterrows() if r['difficulty'] == 'advanced')
    
    print(f"\né”™è¯¯ç±»å‹åˆ†å¸ƒ:")
    print(f"  - è¯­æ³•é”™è¯¯ (Syntax): {syntax_count} ä¸ª")
    print(f"  - è¿è¡Œæ—¶é”™è¯¯ (Runtime): {runtime_count} ä¸ª")
    print(f"  - é€»è¾‘é”™è¯¯ (Logical): {logical_count} ä¸ª")
    print(f"  - æ¦‚å¿µé”™è¯¯ (Conceptual): {conceptual_count} ä¸ª")
    
    print(f"\néš¾åº¦ç­‰çº§åˆ†å¸ƒ:")
    print(f"  - åˆçº§ (Beginner): {beginner_count} ä¸ª")
    print(f"  - ä¸­çº§ (Intermediate): {intermediate_count} ä¸ª")
    print(f"  - é«˜çº§ (Advanced): {advanced_count} ä¸ª")
    
    # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨
    print("\n[é˜¶æ®µ2/4] åˆå§‹åŒ– PhaseEvo ä¼˜åŒ–å™¨...")
    config = {
        'total_generations': 10,  # æ€»è¿­ä»£æ¬¡æ•°
        'population_size': 6,      # ç§ç¾¤å¤§å°ï¼ˆå¢åŠ åˆ°6ä¸ªï¼‰
        'max_tokens': 999999999,   # Tokené¢„ç®—ï¼ˆæ— é™åˆ¶ï¼‰
        'precision_weight': 0.7,   # åå‘å‡†ç¡®æ€§
        'recall_weight': 0.3,
        'eval_set_size': len(test_data),
        
        # é’ˆå¯¹æ•™å­¦åœºæ™¯çš„ç‰¹æ®Šé…ç½®
        'fp_pool_size': 100,       # é”™è¯¯æ ·æœ¬æ± å¤§å°
        'min_precision': 0.85,     # æœ€ä½å‡†ç¡®ç‡è¦æ±‚
        
        # ä¿å®ˆç­–ç•¥
        'conservative_threshold': 0.90,
        'must_include_fp': True,
        'fp_ratio': 0.6,
        
        # å¹¶è¡Œæ§åˆ¶ï¼ˆå‡å°‘APIå¹¶å‘å‹åŠ›ï¼‰
        'max_workers': 1  # ä¸²è¡Œè¯„ä¼°ï¼Œé¿å…APIé™æµ
    }
    
    optimizer = TeachingOptimizer(config)  # ä½¿ç”¨å®šåˆ¶çš„æ•™å­¦ä¼˜åŒ–å™¨
    evaluator = TeachingPromptEvaluator(volcengine_ark_llm_eval)
    
    print("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"\nä¼˜åŒ–é…ç½®:")
    print(f"  - æ€»ä»£æ•°: {config['total_generations']}")
    print(f"  - ç§ç¾¤å¤§å°: {config['population_size']}")
    print(f"  - Tokené¢„ç®—: {config['max_tokens']:,} tokens")
    print(f"  - å‡†ç¡®æ€§æƒé‡: {config['precision_weight']}")
    print(f"  - æœ€ä½å‡†ç¡®ç‡: {config['min_precision']}")
    
    # 3. æ‰§è¡Œä¼˜åŒ–
    print("\n[é˜¶æ®µ3/4] å¼€å§‹è‡ªåŠ¨åŒ–è¿­ä»£ä¼˜åŒ–...")
    print("=" * 70)
    print("é¢„è®¡è€—æ—¶: 15-25åˆ†é’Ÿ")
    print("ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºè¯¦ç»†è¿›åº¦...")
    print("=" * 70)
    
    try:
        best_candidate = optimizer.optimize(
            target_tag="ç¼–ç¨‹æ•™å­¦é”™è¯¯æ£€æµ‹",
            evaluator=evaluator,
            data=test_data
        )
    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. ä¿å­˜ç»“æœ
    print("\n[é˜¶æ®µ4/4] ä¿å­˜ä¼˜åŒ–ç»“æœ...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ„å»ºç»“æœæ•°æ®
    result = {
        'timestamp': timestamp,
        'method': 'PhaseEvo + Autoprompt',
        'optimized_prompt': best_candidate.prompt,
        'score': float(best_candidate.score),
        'metrics': {},
        'optimization_history': optimizer.optimization_history,
        'token_usage': {
            'consumed': optimizer.token_consumed,
            'budget': optimizer.token_budget,
            'usage_ratio': optimizer.token_consumed / optimizer.token_budget if optimizer.token_budget > 0 else 0,
            'llm_calls': optimizer.llm_call_count
        },
        'config': config,
        'test_data_info': {
            'total_samples': len(test_data),
            'syntax_errors': syntax_count,
            'runtime_errors': runtime_count,
            'logical_errors': logical_count,
            'conceptual_errors': conceptual_count,
            'beginner_level': beginner_count,
            'intermediate_level': intermediate_count,
            'advanced_level': advanced_count
        }
    }
    
    # æå–metricsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if best_candidate.evidence and 'metrics' in best_candidate.evidence:
        result['metrics'] = best_candidate.evidence['metrics']
    
    # ä¿å­˜å®Œæ•´ç»“æœï¼ˆJSONï¼‰
    result_file = os.path.join(results_dir, f'optimized_prompt_{timestamp}.json')
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜çº¯æç¤ºè¯æ–‡æœ¬ï¼ˆç”¨äºbackendï¼‰
    prompt_file = os.path.join(results_dir, f'system_prompt_{timestamp}.txt')
    with open(prompt_file, 'w', encoding='utf-8') as f:
        f.write(best_candidate.prompt)
    
    # ä¿å­˜ä¼˜åŒ–å†å²ï¼ˆCSVæ ¼å¼ï¼Œä¾¿äºåˆ†æï¼‰
    if optimizer.optimization_history:
        import pandas as pd
        history_df = pd.DataFrame(optimizer.optimization_history)
        history_file = os.path.join(results_dir, f'optimization_history_{timestamp}.csv')
        history_df.to_csv(history_file, index=False, encoding='utf-8-sig')
        print(f"  - ä¼˜åŒ–å†å²: {history_file}")
    
    print(f"âœ“ ç»“æœå·²ä¿å­˜:")
    print(f"  - å®Œæ•´ç»“æœ: {result_file}")
    print(f"  - ç³»ç»Ÿæç¤ºè¯: {prompt_file}")
    
    # 5. è¾“å‡ºæ‘˜è¦
    print("\n" + "=" * 70)
    print("ä¼˜åŒ–å®Œæˆæ‘˜è¦")
    print("=" * 70)
    print(f"\nğŸ“Š æœ€ç»ˆå¾—åˆ†: {best_candidate.score:.4f}")
    
    if best_candidate.evidence and 'metrics' in best_candidate.evidence:
        metrics = best_candidate.evidence['metrics']
        print(f"\nè¯¦ç»†æŒ‡æ ‡:")
        print(f"  - é”™è¯¯æ£€æµ‹å‡†ç¡®æ€§: {metrics.get('error_detection', 0):.4f}")
        print(f"  - æ•™è‚²ä»·å€¼: {metrics.get('educational_value', 0):.4f}")
        print(f"  - æ ¼å¼è§„èŒƒæ€§: {metrics.get('format_compliance', 0):.4f}")
        print(f"  - éš¾åº¦é€‚åº”æ€§: {metrics.get('difficulty_adaptation', 0):.4f}")
        
        if 'stats' in metrics:
            stats = metrics['stats']
            print(f"\nåˆ†ç±»å‡†ç¡®ç‡:")
            if syntax_count > 0:
                print(f"  - è¯­æ³•é”™è¯¯: {stats.get('syntax_correct', 0)}/{syntax_count} ({stats.get('syntax_correct', 0)/syntax_count*100:.1f}%)")
            if runtime_count > 0:
                print(f"  - è¿è¡Œæ—¶é”™è¯¯: {stats.get('runtime_correct', 0)}/{runtime_count} ({stats.get('runtime_correct', 0)/runtime_count*100:.1f}%)")
            if logical_count > 0:
                print(f"  - é€»è¾‘é”™è¯¯: {stats.get('logical_correct', 0)}/{logical_count} ({stats.get('logical_correct', 0)/logical_count*100:.1f}%)")
            if conceptual_count > 0:
                print(f"  - æ¦‚å¿µé”™è¯¯: {stats.get('conceptual_correct', 0)}/{conceptual_count} ({stats.get('conceptual_correct', 0)/conceptual_count*100:.1f}%)")
    
    print(f"\nğŸ’° Tokenä½¿ç”¨:")
    print(f"  - æ¶ˆè€—: {optimizer.token_consumed:,} / {optimizer.token_budget:,}")
    print(f"  - ä½¿ç”¨ç‡: {optimizer.token_consumed/optimizer.token_budget*100:.1f}%")
    print(f"  - LLMè°ƒç”¨æ¬¡æ•°: {optimizer.llm_call_count}")
    if optimizer.llm_call_count > 0:
        print(f"  - å¹³å‡æ¯æ¬¡è°ƒç”¨: {optimizer.token_consumed/optimizer.llm_call_count:.0f} tokens")
    
    print(f"\nğŸ¯ ä¼˜åŒ–è¿­ä»£:")
    print(f"  - æ€»ä»£æ•°: {len(optimizer.optimization_history)}")
    if optimizer.optimization_history:
        best_gen = max(optimizer.optimization_history, key=lambda x: x.get('best_score', 0))
        print(f"  - æœ€ä½³ä»£æ•°: ç¬¬{best_gen.get('generation', 0)+1}ä»£")
        print(f"  - æœ€ä½³å¾—åˆ†: {best_gen.get('best_score', 0):.4f}")
    
    print("\n" + "=" * 70)
    print("âœ… ä¼˜åŒ–åçš„æç¤ºè¯å¯ç›´æ¥ç”¨äº backend/server.js")
    print(f"ğŸ“ æç¤ºè¯æ–‡ä»¶: {prompt_file}")
    print("=" * 70)
    
    # æ˜¾ç¤ºä¼˜åŒ–åçš„æç¤ºè¯é¢„è§ˆ
    print(f"\nğŸ“ ä¼˜åŒ–åçš„æç¤ºè¯é¢„è§ˆï¼ˆå‰300å­—ç¬¦ï¼‰:")
    print("-" * 70)
    print(best_candidate.prompt[:300] + "...")
    print("-" * 70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¼˜åŒ–è¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

